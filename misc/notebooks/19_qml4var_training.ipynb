{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42da328f",
   "metadata": {},
   "source": [
    "# Training the PQCs\n",
    "\n",
    "Now all the mandatory ingredients for training a **PQC** for a **CDF** surrogate model are ready. The user can build any training workflow with the functions from **QQuantLib.qml4var** package. \n",
    "\n",
    "Present notebook builds this training workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423675ec",
   "metadata": {},
   "source": [
    "## 1. The Optimizer\n",
    "\n",
    "Before building the training workflow a mandatory ingredient is an **Optimizer** for updating the trainable parameters of the **PQC** ($\\theta$). \n",
    "\n",
    "Any **Optimizer** software can be used (the workflows that compute the desired *Loss Function* and their gradients should be provided).\n",
    "\n",
    "In Deep Learning, one of the most popular **Optimizers** is the *ADAM* one. A minimum working implementation of the *ADAM* is provided in the **QQuantLib.qml4var.adam** module in the function *adam_optimizer_loop*.\n",
    "\n",
    "The main inputs of the function are:\n",
    "\n",
    "* weights_dict: dictionary where the keys are the different **PQC** parameter related to the **weitghs** ($\\theta$).\n",
    "* loss_function: properly configurated workflow for computing the desired *Loss function* to optimize.\n",
    "* metric_function: properly configurated workflow for computing a desired *metric* for monitoring purpouses (not mandatory).\n",
    "* gradient_function: properly configurated workflow for computing the gradients of the *Loss function* to optimize.\n",
    "* batch_generator: function for generating batches of the training data.\n",
    "* initial_time: initial epoch.\n",
    "\n",
    "In addition to these input a keyword arguments can be provided to the function. For configuring the *ADAM* optimizer the following keywords can be provided:\n",
    "\n",
    "* learning_rate : learning_rate for ADAM.\n",
    "* beta1 : beta1 for ADAM.\n",
    "* beta2 : beta2 for ADAM.\n",
    "\n",
    "Additionally, the training loop can be configured using the following arguments:\n",
    "\n",
    "* epochs: maximum number of iterations.\n",
    "* print_step: print_step for printing evolution of training (the evaluation of the *Loss function* and the *metric* will be printed.\n",
    "* tolerance: tolerance to achieve. This parameter is used with the *n_counts_tolerance*.\n",
    "* n_counts_tolerance: number of times the tolerance should be achieved in consecutive iterations.\n",
    "\n",
    "The *tolerance* and the *n_counts_tolerance* can be used to stop the training loop before the number of desired epochs is achieved. The training will be stopped when the computed tolerance, that is the difference between the *Loss function* after and before a weight updating, be lower than the *tolerance* in *n_counts_tolerance* consecutive steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb587d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import json\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cf1fd",
   "metadata": {},
   "source": [
    "## 2. Example of a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5008a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2f338",
   "metadata": {},
   "source": [
    "### 2.0 Dask Client \n",
    "\n",
    "A *DASK* client can be provided to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88611f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "dask_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f61b8",
   "metadata": {},
   "source": [
    "### 2.1 Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.qml4var.data_sets import create_random_data\n",
    "cfg_random = {\n",
    "    \"n_points_train\": 10, \n",
    "    \"n_points_test\" : 100,\n",
    "    \"minval\" : -np.pi,\n",
    "    \"maxval\" : np.pi,\n",
    "    \"features_number\" : 1\n",
    "}\n",
    "x_train, y_train, x_test, y_test = create_random_data(\n",
    "    **cfg_random\n",
    ")\n",
    "plt.plot(x_train, y_train, \"o\")\n",
    "plt.plot(x_test, y_test, \"-\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend([\"Training Dataset\", \"Testing Dataset\"])\n",
    "plt.title(\"Random Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84d143",
   "metadata": {},
   "source": [
    "### 2.2 Build PQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.architectures import hardware_efficient_ansatz, z_observable, normalize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f76610",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqc_cfg = {\n",
    "    \"features_number\" : cfg_random[\"features_number\"],\n",
    "    \"n_qubits_by_feature\" : 2,\n",
    "    \"n_layers\": 3    \n",
    "}\n",
    "# Normalization function\n",
    "base_frecuency, shift_feature = normalize_data(\n",
    "    [cfg_random[\"minval\"]] * cfg_random[\"features_number\"],\n",
    "    [cfg_random[\"maxval\"]] * cfg_random[\"features_number\"],\n",
    "    [-0.5*np.pi] * cfg_random[\"features_number\"],\n",
    "    [0.5*np.pi] * cfg_random[\"features_number\"]   \n",
    ")\n",
    "pqc_cfg.update({\n",
    "    \"base_frecuency\" : base_frecuency,\n",
    "    \"shift_feature\" : shift_feature    \n",
    "})   \n",
    "print(pqc_cfg)\n",
    "pqc, weights_names, features_names = hardware_efficient_ansatz(**pqc_cfg)\n",
    "observable = z_observable(**pqc_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PQC\n",
    "circuit = pqc.to_circ()\n",
    "circuit.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dabe7c",
   "metadata": {},
   "source": [
    "### 2.3 QPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.utils.benchmark_utils import combination_for_list\n",
    "from QQuantLib.qpu.select_qpu import select_qpu\n",
    "json_qpu = \"../../benchmark/qml4var/JSONs/qpu_ideal.json\"\n",
    "with open(json_qpu) as json_file:\n",
    "    qpu_dict = json.load(json_file)\n",
    "qpu_list = combination_for_list(qpu_dict)\n",
    "qpu_dict = qpu_list[0]\n",
    "print(qpu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee2948",
   "metadata": {},
   "source": [
    "### 2.4 Configure Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import qdml_loss_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for workflows\n",
    "nbshots = 0\n",
    "# Discretization domain intervals\n",
    "points = 100\n",
    "workflow_cfg = {\n",
    "    \"pqc\" : pqc,\n",
    "    \"observable\" : observable,\n",
    "    \"weights_names\" : weights_names,\n",
    "    \"features_names\" : features_names,\n",
    "    \"minval\" : [cfg_random[\"minval\"]] * cfg_random[\"features_number\"],\n",
    "    \"maxval\" : [cfg_random[\"maxval\"]] * cfg_random[\"features_number\"],\n",
    "    \"nbshots\" : nbshots,\n",
    "    \"points\" : points,\n",
    "    \"qpu_info\" : qpu_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the training function should be computed\n",
    "training_loss = lambda w_: qdml_loss_workflow(\n",
    "    w_, x_train, y_train, dask_client=dask_client, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554af6c",
   "metadata": {},
   "source": [
    "### 2.5 Configure the Loss function gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404888a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.losses import numeric_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdml_loss_workflow_ = lambda w_, x_, y_: qdml_loss_workflow(\n",
    "    w_, x_, y_, dask_client=dask_client, **workflow_cfg)\n",
    "numeric_gradient_ = lambda w_, x_, y_: numeric_gradient(\n",
    "    w_, x_, y_, qdml_loss_workflow_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717f088",
   "metadata": {},
   "source": [
    "### 2.6 Configure a Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import mse_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5324c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_metric = lambda w_: mse_workflow(\n",
    "    w_, x_test, y_test, dask_client=dask_client, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd0e9d",
   "metadata": {},
   "source": [
    "### 2.7 Configure a Batch Generator\n",
    "\n",
    "A batch generator for the data should be built. The main function is splitting the input training data in batches. The updating of the weights will be done after each batch is processed.\n",
    "\n",
    "The following code implements a very basic batch generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292956ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size):\n",
    "    return [(X[i:i+batch_size] , Y[i:i+batch_size]) for i in range(0, len(X), batch_size)]\n",
    "batch_size = None\n",
    "if batch_size is None:\n",
    "    batch_size = len(x_train)\n",
    "\n",
    "batch_generator_ = batch_generator(x_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1328fa8",
   "metadata": {},
   "source": [
    "## 2.8 Training Time\n",
    "\n",
    "Now all the ingredients are ready. We need to initialize weights (the *init_weights* function from **QQuantLib.qml4var.architectures** module can be used), configures the optimizer and use the *adam_optimizer_loop* from **QQuantLib.qml4var.adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbdced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.architectures import init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b323808",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = init_weights(weights_names)\n",
    "print(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398f553",
   "metadata": {},
   "source": [
    "The following optimizer configuration will be used. We are going to train for 100 epochs, but if the tolerance decreases below $10^{-5}$ during 10 consecutive epochs the training should be stopped. Additionally, training information will be printed each 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ba75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "optimizer_cfg = {\n",
    "    \"epochs\" : 50,\n",
    "    \"tolerance\" : 1.0e-5,\n",
    "    \"print_step\" : 10,\n",
    "    \"n_counts_tolerance\" : 10\n",
    "}\n",
    "# ADAM configuration\n",
    "optimizer_cfg.update({\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"beta1\" : 0.9,\n",
    "    \"beta2\" : 0.999,       \n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.adam import adam_optimizer_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = adam_optimizer_loop(\n",
    "    weights_dict=initial_weights,\n",
    "    loss_function=training_loss,\n",
    "    metric_function=testing_metric,\n",
    "    gradient_function=numeric_gradient_,\n",
    "    batch_generator=batch_generator_,\n",
    "    initial_time=0,\n",
    "    **optimizer_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e4333",
   "metadata": {},
   "source": [
    "## 3. Results of the Training\n",
    "\n",
    "We can use the updated weights for evaluate the **PQC** performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73296d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import workflow_for_cdf, workflow_for_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53683a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_prediction_train = workflow_for_cdf(\n",
    "    weights_0, x_train, dask_client=dask_client, **workflow_cfg)[\"y_predict_cdf\"]\n",
    "pdf_prediction_train = workflow_for_pdf(\n",
    "    weights_0, x_train, dask_client=dask_client, **workflow_cfg)[\"y_predict_pdf\"]\n",
    "cdf_prediction_test = workflow_for_cdf(\n",
    "    weights_0, x_test, dask_client=dask_client, **workflow_cfg)[\"y_predict_cdf\"]\n",
    "pdf_prediction_test = workflow_for_pdf(\n",
    "    weights_0, x_test, dask_client=dask_client, **workflow_cfg)[\"y_predict_pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56829c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, \"o\")\n",
    "plt.plot(x_train, cdf_prediction_train, \"o\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend([\"Train Data\", \"Trained PQC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test, \"-\")\n",
    "plt.plot(x_test, cdf_prediction_test, \"o\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend([\"Test Data\", \"Trained PQC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a98ed9",
   "metadata": {},
   "source": [
    "We can easily continue the training by providing the last iteration, updating the *optimizer_cfg* and provided the last obtained weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cfg.update({\"epochs\" : 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0_d = dict(zip(weights_names,weights_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f65f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_0_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = adam_optimizer_loop(\n",
    "    weights_dict=weights_0_d,\n",
    "    loss_function=training_loss,\n",
    "    metric_function=testing_metric,\n",
    "    gradient_function=numeric_gradient_,\n",
    "    batch_generator=batch_generator_,\n",
    "    initial_time=50,\n",
    "    **optimizer_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_prediction_train = workflow_for_cdf(\n",
    "    weights, x_train, dask_client=dask_client, **workflow_cfg)[\"y_predict_cdf\"]\n",
    "pdf_prediction_train = workflow_for_pdf(\n",
    "    weights, x_train, dask_client=dask_client, **workflow_cfg)[\"y_predict_pdf\"]\n",
    "cdf_prediction_test = workflow_for_cdf(\n",
    "    weights, x_test, dask_client=dask_client, **workflow_cfg)[\"y_predict_cdf\"]\n",
    "pdf_prediction_test = workflow_for_pdf(\n",
    "    weights, x_test, dask_client=dask_client, **workflow_cfg)[\"y_predict_pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, \"o\")\n",
    "plt.plot(x_train, cdf_prediction_train, \"o\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend([\"Train Data\", \"Trained PQC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8871e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test, \"-\")\n",
    "plt.plot(x_test, cdf_prediction_test, \"o\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend([\"Test Data\", \"Trained PQC\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

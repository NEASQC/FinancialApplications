{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855eaacd",
   "metadata": {},
   "source": [
    "# Computing Loss Function and Gradients\n",
    "\n",
    "In this notebook, we will finalize the training workflow for the **PQC** by defining the *Loss function* and the corresponding gradient computations. This is the last mandatory step before training the **PQC**.\n",
    "\n",
    "We already have the following components:\n",
    "\n",
    "* A dataset (generated using *15_qml4var_DataSets.ipynb*).\n",
    "* A PQC architecture (from *16_qml4var_BuildPQC.ipynb*).\n",
    "* A PQC evaluation workflow (from *17_qml4var_pqc_evaluation.ipynb*), which allows us to compute the output of the PQC for given parameters $\\theta$ and input **features**.\n",
    "\n",
    "\n",
    "Before explaining what *Loss function* we will use and how to implement it using the available functions, we need to get some data and configure a **PQC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import json\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a663f5c",
   "metadata": {},
   "source": [
    "Before explaining the loss function we are going to create a dataset and a **PQC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256fb84",
   "metadata": {},
   "source": [
    "## 1. Get some dataset\n",
    "\n",
    "First, some dataset for evaluating the **PQC** is needed. A random dataset will be generated using the *create_random_data* from **benchmark.qml4var.data_sets** module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.qml4var.data_sets import create_random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_random = {\n",
    "    \"n_points_train\": 10, \n",
    "    \"n_points_test\" : 100,\n",
    "    \"minval\" : -np.pi,\n",
    "    \"maxval\" : np.pi,\n",
    "    \"features_number\" : 1\n",
    "}\n",
    "x_train, y_train, x_test, y_test = create_random_data(\n",
    "    **cfg_random\n",
    ")\n",
    "plt.plot(x_train, y_train, \"o\")\n",
    "plt.plot(x_test, y_test, \"-\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend([\"Training Dataset\", \"Testing Dataset\"])\n",
    "plt.title(\"Random Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c0261",
   "metadata": {},
   "source": [
    "## 2. Build the PQC\n",
    "\n",
    "The *hardware_efficient_ansatz* and the *z_observable* from **QQuantLib.qml4var.architectures** modules will be used to build the **PQC**. Additionally, the *normalize_data* function will be used for data normalization between $\\frac{-\\pi}{2}$ and $\\frac{\\pi}{2}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.architectures import hardware_efficient_ansatz, z_observable, normalize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ddd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pqc_cfg = {\n",
    "    \"features_number\" : cfg_random[\"features_number\"],\n",
    "    \"n_qubits_by_feature\" : 2,\n",
    "    \"n_layers\": 3    \n",
    "}\n",
    "# Normalization function\n",
    "base_frecuency, shift_feature = normalize_data(\n",
    "    [cfg_random[\"minval\"]] * cfg_random[\"features_number\"],\n",
    "    [cfg_random[\"maxval\"]] * cfg_random[\"features_number\"],\n",
    "    [-0.5*np.pi] * cfg_random[\"features_number\"],\n",
    "    [0.5*np.pi] * cfg_random[\"features_number\"]   \n",
    ")\n",
    "pqc_cfg.update({\n",
    "    \"base_frecuency\" : base_frecuency,\n",
    "    \"shift_feature\" : shift_feature    \n",
    "})   \n",
    "print(pqc_cfg)\n",
    "pqc, weights_names, features_names = hardware_efficient_ansatz(**pqc_cfg)\n",
    "observable = z_observable(**pqc_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PQC\n",
    "circuit = pqc.to_circ()\n",
    "circuit.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982eecc",
   "metadata": {},
   "source": [
    "## 3. QPU info\n",
    "\n",
    "To evaluate the different quantum circuits,  a **myQLM QPU** is needed. In the following cell, the configuration for it is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.utils.benchmark_utils import combination_for_list\n",
    "from QQuantLib.qpu.select_qpu import select_qpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e33d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_qpu = \"../../benchmark/qml4var/JSONs/qpu_ideal.json\"\n",
    "with open(json_qpu) as json_file:\n",
    "    qpu_dict = json.load(json_file)\n",
    "qpu_list = combination_for_list(qpu_dict)\n",
    "qpu_dict = qpu_list[0]\n",
    "print(qpu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03205ec",
   "metadata": {},
   "source": [
    "## 4. Get the workflow evaluation functions\n",
    "\n",
    "To compute the *Loss function* the different workflows for evaluating the **CDF** and the **PDF** using **PQC**s will be needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d21da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import cdf_workflow, pdf_workflow, workflow_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98581e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for workflows\n",
    "nbshots = 0\n",
    "workflow_cfg = {\n",
    "    \"pqc\" : pqc,\n",
    "    \"observable\" : observable,\n",
    "    \"weights_names\" : weights_names,\n",
    "    \"features_names\" : features_names,\n",
    "    \"nbshots\" : nbshots,\n",
    "    \"qpu_info\" : qpu_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First configure porperly the desired workflow computation using Python lambdas\n",
    "\n",
    "# For computing CDF using PQC\n",
    "cdf_workflow_ = lambda w,x : cdf_workflow(w, x, **workflow_cfg)\n",
    "# For computing PDF using PQC\n",
    "pdf_workflow_ = lambda w,x : pdf_workflow(w, x, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd08f9d",
   "metadata": {},
   "source": [
    "## 5. The Loss function\n",
    "\n",
    "\n",
    "The main objective is to train a **PQC** represented by $F^*(\\textbf{x}, \\theta)$ using data sampled from a **CDF** financial distribution: $\\tilde{\\textbf{x}}^j \\sim F(\\textbf{x})$ in such a way that the trained $F^*(\\textbf{x}, \\theta)$ can be used as a surrogate model of the $F(\\textbf{x})$ for **VaR** computations.\n",
    "\n",
    "The *Loss function* we are going to use to achieve this objective will be the following one:\n",
    "\n",
    "$$R_{L^2, \\bar{L}^2}^{S_{\\chi}} = \\alpha_0 \\frac{1}{m}\\sum_{j=0}^{m-1} (cdf(\\tilde{\\textbf{x}}^j; \\theta) -y^j)^2 + \\alpha_1 \\left(-\\frac{1}{m}\\sum_{j=0}^{m-1} pdf(\\tilde{\\textbf{x}}^j; \\vec{\\theta})  + \\int_{\\textbf{x}_{min}}^{\\textbf{x}_{max}} pdf^2(\\textbf{x}; \\theta) d\\textbf{x} \\right) $$\n",
    "\n",
    "Where:\n",
    " \n",
    "* $cdf(\\tilde{\\textbf{x}}^j; \\theta)$ is the **CDF** computed by the *PQC* evaluated in the training data $\\tilde{\\textbf{x}}^j$.\n",
    "* $pdf(\\tilde{\\textbf{x}}^j; \\theta)$ is the **PDF** computed using the *PQC* evaluated in the training data $\\tilde{\\textbf{x}}^j$.\n",
    "\n",
    "The integral can be evaluated numerically by discretizing the complete domain and computing for each point $\\textbf{x}^k$ the $pdf(\\textbf{x}^k; \\theta)$\n",
    "\n",
    "This loss is defined into the function *loss_function_qdml* from the **QQuantLib.qml4var.losses** module. The mandatory inputs are:\n",
    "\n",
    "* labels: they are the $y^j$\n",
    "* predict_cdf : they are the $cdf(\\textbf{x}^j; \\theta)$\n",
    "* predict_pdf : they are the $pdf(\\textbf{x}^j; \\theta)$\n",
    "* integral: the evaluation of the integral:$\\int_{\\textbf{x}_{min}}^{\\textbf{x}_{max}} pdf^2(\\textbf{x}; \\theta) d\\textbf{x} $\n",
    "* loss_weights: list wit the weights for each part of the loss function: [$\\alpha_0$, $\\alpha_1$]\n",
    "\n",
    "All this inputs can be computed using the *workflow_execution* (from **QQuantLib.qml4var.qlm_procces**) using a properly configured *cdf_workflow* and *pdf_workflow* functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.losses import loss_function_qdml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23815a",
   "metadata": {},
   "source": [
    "### 5.1 Computation of the inputs for loss_function_qdml\n",
    "\n",
    "\n",
    "In the following cells, the computations for $cdf(\\tilde{\\textbf{x}}^j; \\theta)$  and for $pdf(\\tilde{\\textbf{x}}^j; \\theta)$ are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb87e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialize weights\n",
    "weights_= [np.random.rand() for w in weights_names]\n",
    "#get cdf evaluated over the training dataset\n",
    "cdf_prediction = np.array(workflow_execution(weights_, x_train, cdf_workflow_))\n",
    "#get pdf evaluated over the training dataset\n",
    "pdf_prediction = np.array(workflow_execution(weights_, x_train, pdf_workflow_))\n",
    "# Rearrange using the shape of the labels\n",
    "cdf_prediction = cdf_prediction.reshape(y_train.shape)\n",
    "pdf_prediction = pdf_prediction.reshape(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f5516",
   "metadata": {},
   "source": [
    "Now we need to build the domain for computing the mandatory integral for creating the desired *Loss function*. \n",
    "\n",
    "The domain will be discretized over a given number of points for each axis (i.e. for each possible feature). For more than 1 feature, the domain will be the cartesian product of the discretization over all the features (we are going to build a complete mesh over all the n-dimensional domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization over each feature domain.\n",
    "discretization_points = 100\n",
    "domain_x = np.linspace(\n",
    "    [cfg_random[\"minval\"]] * cfg_random[\"features_number\"],\n",
    "    [cfg_random[\"maxval\"]] * cfg_random[\"features_number\"],\n",
    "    discretization_points\n",
    ")\n",
    "# Cartesian product for building a mesh over the n-dimensional domain\n",
    "domain_x = np.array(list(\n",
    "    product(*[domain_x[:, i] for i in range(domain_x.shape[1])])\n",
    "))\n",
    "domain_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf829b",
   "metadata": {},
   "source": [
    "Now we are going to obtain the **PDF** evaluation for each n-dimensional point of the domain! For this a new *workflow* will be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400706bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_workflow_square = lambda w, x: pdf_workflow(w, x, **workflow_cfg) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86afe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of PDF using x_quad: \n",
    "pdf_squqare_domain_prediction = np.array(workflow_execution(weights_, domain_x, pdf_workflow_square))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed61ac5",
   "metadata": {},
   "source": [
    "#### Computing the integral\n",
    "\n",
    "Now we have all ingredients for computing the integral mandatory for computing the *loss function*. The function *compute_integral* from **QQuantLib.qml4var.losses** module can be used for computing it. \n",
    "\n",
    "The inputs of the function are:\n",
    "\n",
    "* y_array: numpy array or list of dask futures with the y for integral computation\n",
    "    * For numpy array expected shape is: (n, 1)\n",
    "* x_array: numpy array with the domain for the numerical integration:\n",
    "    * Expected shape: (n, features)\n",
    "* dask_client: Dask client to speed up computations.\n",
    "\n",
    "The output will be a float if dask_client is not provided and a future otherwise.\n",
    "\n",
    "**Computation considerations**\n",
    "\n",
    "The integral computation will depend on the inputs:\n",
    "* If x_array.shape == (n, 1) then np.trapz is used for integral computation.\n",
    "* If x_array.shape == (n, 2) AND dask_client is not provided then the double integration is performed using np.trapz by meshgrid properly the domain and doing the corresponding reshape of the y_array\n",
    "* If x_array.shape == (n, 2) AND dask_client is provided then the integration is computed using the MonteCarlo integration method.\n",
    "* If x_array.shape == (n, >2) then the integration is computed using the MonteCarlo integration method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.losses import compute_integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65306264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_integral(y_array, x_array, dask_client=None):\n",
    "    \"\"\"\n",
    "    Function for computing numerical integral of inputs arrays. Considerations:\n",
    "    * if x_array has shape(n, 1) then numpy trapz is used for computing integral.\n",
    "    * if x_array has shape(n, 2) and dask_client is None numpy trapz\n",
    "        is used for computing the double integral.\n",
    "    * if x_array has shape(n, 2) and dask_client is provided then MonteCarlo\n",
    "        integration is used\n",
    "    * if x_array has shape(n, > 2) MonteCarlo integration is used\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_array : np.array or dask futures:\n",
    "        array or futures (only is dask_client is provided) with the\n",
    "        y-values for integration.  For array expected shape is: shape(n)\n",
    "    x_array : np array\n",
    "        array with the x domain for integration: Shape(n, n_features)\n",
    "    dask_client : DASK client\n",
    "        DASK client to submit computation of the integral.\n",
    "        y_array MUST BE a list of futures\n",
    "    Returns\n",
    "    -------\n",
    "    integral : float or future\n",
    "        float or future (if dask_client is provided) with the desired\n",
    "        integral computation.\n",
    "    \"\"\"\n",
    "    if x_array.shape[1] == 1:\n",
    "        if dask_client is None:\n",
    "            integral = np.trapz(y=y_array, x=x_array[:, 0])\n",
    "        else:\n",
    "            integral = dask_client.submit(np.trapz, y_array, x_array[:, 0])\n",
    "    elif x_array.shape[1] == 2:\n",
    "        if dask_client is None:\n",
    "            x_domain, y_domain = np.meshgrid(\n",
    "                np.unique(x_array[:, 0]),\n",
    "                np.unique(x_array[:, 1])\n",
    "            )\n",
    "            y_array_ = y_array.reshape(x_domain.shape)\n",
    "            integral = np.trapz(\n",
    "                np.trapz(y=y_array_, x=x_domain),\n",
    "                x=y_domain[:, 0]\n",
    "            )\n",
    "        else:\n",
    "            # MonteCarlo integration\n",
    "            factor = np.prod(x_array.max(axis=0) - x_array.min(axis=0)) / len(y_array)\n",
    "            integral = dask_client.submit(\n",
    "                lambda x: np.sum(x) * factor,\n",
    "                y_array\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        # MonteCarlo integration\n",
    "        if dask_client is None:\n",
    "            factor = np.prod(x_array.max(axis=0) - x_array.min(axis=0)) / y_array.size\n",
    "            integral = np.sum(y_array) * factor\n",
    "        else:\n",
    "            factor = np.prod(x_array.max(axis=0) - x_array.min(axis=0)) / len(y_array)\n",
    "            integral = dask_client.submit(\n",
    "                lambda x: np.sum(x) * factor,\n",
    "                y_array\n",
    "            )\n",
    "    return integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "integral = compute_integral(pdf_squqare_domain_prediction, domain_x)\n",
    "print(\"integral : {}\".format(integral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = loss_function_qdml(\n",
    "    y_train, cdf_prediction, pdf_prediction, integral\n",
    ")\n",
    "print(\"The computed Loss is: {}\".format(loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ec1e9",
   "metadata": {},
   "source": [
    "#### Using Dask Client\n",
    "\n",
    "The number of quantum circuits evaluations can scale very quicly (especially if more tha 2 features are used). If it is available a *DASK* cluster can be used to speed up this computations. As explained in notebook *17_qml4var_pqc_evaluation.ipynb* the *DASK* client should be provided to the *workflow_execution* function!!\n",
    "\n",
    "\n",
    "**BE AWARE**\n",
    "\n",
    "The following cells should be executed only if a *DASK* cluster is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "path_to_schedule_json = \"/home/cesga/gferro/Codigo/qlm_cVar/dask_cluster_ft3/scheduler_info.json\"\n",
    "#path_to_schedule_json = \"/home/cesga/gferro/Codigo/dask_cluster_ft3/scheduler_info.json\"\n",
    "client = Client(\n",
    "    scheduler_file = path_to_schedule_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84392f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#get cdf evaluated over the training dataset\n",
    "cdf_prediction_dask = workflow_execution(weights_, x_train, cdf_workflow_, client)\n",
    "#get pdf evaluated over the training dataset\n",
    "pdf_prediction_dask = workflow_execution(weights_, x_train, pdf_workflow_, client)\n",
    "# Discretization over each feature domain.\n",
    "discretization_points = 100\n",
    "domain_x = np.linspace(\n",
    "    [cfg_random[\"minval\"]] * cfg_random[\"features_number\"],\n",
    "    [cfg_random[\"maxval\"]] * cfg_random[\"features_number\"],\n",
    "    discretization_points\n",
    ")\n",
    "# Cartesian product for building a mesh over the n-dimensional domain\n",
    "domain_x = np.array(list(\n",
    "    product(*[domain_x[:, i] for i in range(domain_x.shape[1])])\n",
    "))\n",
    "\n",
    "# Prediction for integration\n",
    "pdf_squqare_domain_prediction_dask = workflow_execution(weights_, domain_x, pdf_workflow_square, client)\n",
    "# The compute_integral need the futures in this case\n",
    "dask_integral = compute_integral(pdf_squqare_domain_prediction_dask, domain_x, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IT IS A FUTURE\n",
    "dask_integral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111edcf4",
   "metadata": {},
   "source": [
    "Once all the futures are obtained we need to gather all the features!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70908981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf_prediction_dask, pdf_prediction_dask and pdf_domain_prediction_dask are dask futures. We need to\n",
    "# retrieve the results of the computations from dask cluster using a gather\n",
    "\n",
    "cdf_prediction_dask = np.array(client.gather(cdf_prediction_dask))\n",
    "pdf_prediction_dask = np.array(client.gather(pdf_prediction_dask))\n",
    "cdf_prediction_dask = cdf_prediction_dask.reshape(y_train.shape)\n",
    "pdf_prediction_dask = pdf_prediction_dask.reshape(y_train.shape)\n",
    "\n",
    "# Additionally we retrieve the integral\n",
    "dask_integral = client.gather(dask_integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336c1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.isclose(cdf_prediction, cdf_prediction_dask).all())\n",
    "print(np.isclose(pdf_prediction, pdf_prediction_dask).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"integral: {}. dask_integral: {}\".format(integral, dask_integral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the Loss function can be computed\n",
    "loss_from_dask = loss_function_qdml(\n",
    "    y_train, cdf_prediction_dask, pdf_prediction_dask, dask_integral\n",
    ")\n",
    "print(\"The computed Loss is: {}\".format(loss_from_dask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba71b7",
   "metadata": {},
   "source": [
    "### 5.2 The workflow_for_qdml function\n",
    "\n",
    "The *workflow_for_qdml* function from **QQuantLib.qml4var.myqlm_workflows** module, implements the before explained scheme to obtain easily the mandatory inputs for the *loss_function_qdml*.\n",
    "\n",
    "The inputs are:\n",
    "* weights: The weights for the **PQC** ($\\theta$).\n",
    "* data_x: numpy array with the training features ($\\{\\tilde{\\textbf{x}}^j ; j=0, 1,\\cdots m-1\\}$).\n",
    "    * Shape: (-1, number of features)\n",
    "* data_y:numpy array with the training features or targets.\n",
    "    * Shape: (-1, number of 1). \n",
    "* kwargs : Keyword arguments with additional information. Mandatory  keywords:\n",
    "    * pqc: The value MUST BE the **myQLM Program** that implements the desired PQC.\n",
    "    * observable: The value MUST BE the **myQLM Observable** for the PQC.\n",
    "    * weights_names: The value MUST BE a list of all the parameter names of the **PQC** related to the weights.\n",
    "    * features_names: The value MUST BE a list of all the parameter names of the **PQC** related to the features.\n",
    "    * nbshots: The number of shots for evaluating the **PQC**.\n",
    "    * qpu_info: configuration dictionary for QPU\n",
    "    * minval: list with minimum values of the domain for all the features.\n",
    "    * maxval: list with maximum values of the domain for all the features.\n",
    "    * points: discretization number of points for the domain of 1 feature.\n",
    "\n",
    "The return of a function will be a Python dictionary with the following keys:\n",
    "\n",
    "* data_y : input data_y data\n",
    "* y_predict_cdf : CDF prediction for data_x\n",
    "* y_predict_pdf : PDF prediction for data_x\n",
    "* integral : the computed integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import workflow_for_qdml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretization_points = 100\n",
    "workflow_cfg = {\n",
    "    \"pqc\" : pqc,\n",
    "    \"observable\" : observable,\n",
    "    \"weights_names\" : weights_names,\n",
    "    \"features_names\" : features_names,\n",
    "    \"nbshots\" : nbshots,\n",
    "    \"qpu_info\" : qpu_dict,\n",
    "    \"minval\" : [cfg_random[\"minval\"]] * cfg_random[\"features_number\"],\n",
    "    \"maxval\" : [cfg_random[\"maxval\"]] * cfg_random[\"features_number\"],\n",
    "    \"points\" : discretization_points,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = workflow_for_qdml(weights_, x_train, y_train, **workflow_cfg)\n",
    "cdf_tp = data[\"y_predict_cdf\"]\n",
    "pdf_tp = data[\"y_predict_pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b69df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(cdf_tp, cdf_prediction).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(pdf_tp, pdf_prediction).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(data[\"integral\"], integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027deb1",
   "metadata": {},
   "source": [
    "#### DASK client\n",
    "\n",
    "To the **workflow_for_qdml** a **DASK** client can be passed to speed up computation.\n",
    "\n",
    "**BE AWARE**\n",
    "\n",
    "The following cells should be executed only if a Dask cluster is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_dask = workflow_for_qdml(weights_, x_train, y_train, dask_client=client, **workflow_cfg)\n",
    "cdf_tp_dask = data_dask[\"y_predict_cdf\"]\n",
    "pdf_tp_dask = data_dask[\"y_predict_pdf\"]\n",
    "integral_dask = data_dask[\"integral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2778a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(cdf_tp, cdf_tp_dask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(pdf_tp, pdf_tp_dask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(data[\"integral\"], integral_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e0f51",
   "metadata": {},
   "source": [
    "### 5.3. qdml_loss_workflow function\n",
    "\n",
    "To compute directly the desired *Loss function* the *qdml_loss_workflow* function from QQuantLib.qml4var.myqlm_workflows. This function uses the *workflow_for_qdml* function and pass the outputs to the *loss_function_qdml* from **QQuantLib.qml4var.losses** module. Do the loss computation can be done transparently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a03fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import qdml_loss_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_ = qdml_loss_workflow(weights_, x_train, y_train, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e067836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss function: {}\".format(loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f1a83",
   "metadata": {},
   "source": [
    "#### DASK client\n",
    "\n",
    "To the **qdml_loss_workflow** a **DASK** client can be passed to speed up computation.\n",
    "\n",
    "**BE AWARE**\n",
    "\n",
    "The following cells should be executed only if a *DASK* cluster is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_dask = qdml_loss_workflow(weights_, x_train, y_train, dask_client=client, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf67bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss function using dask: {}\".format(loss_dask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8227d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad5d31",
   "metadata": {},
   "source": [
    "### 5.4 The mse_workflow function\n",
    "\n",
    "Using the explained procedures along this notebook the user can define their own losses and use the different functions for building a *workflow* for evaluating them. The *mse_workflow* from **QQuantLib.qml4var.myqlm_workflows** builds this *workflow* for computing directly a *Mean Square Error* loss function.\n",
    "\n",
    "(The *DASK* client can be provided to the *mse_workflow* function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff862344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import mse_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ = mse_workflow(weights_, x_train, y_train, dask_client=None, **workflow_cfg)\n",
    "print(mse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ea286",
   "metadata": {},
   "source": [
    "## 6. Numeric Gradients\n",
    "\n",
    "For training the **PQC** in adddition to a *Loss function* a function for computing gradients it is mandatory. \n",
    "\n",
    "In our code, the user can use the *numeric_gradient* from **QQuantLib.qml4var.losses** module. This function allows to compute the gradients of a properly configured loss.\n",
    "\n",
    "The inputs are:\n",
    "\n",
    "* weights : this is the $\\vec{\\theta}$ for the **PQC**\n",
    "* data_x : the dataset with the features: $\\vec{x}^j$\n",
    "* data_y : the labels of the dataset: $y^j$\n",
    "* loss: this is a loss function (like the *mse_workflow* or the *qdml_loss_workflow* from **training_functions**) properly configured. This function only should recived *weights*, *data_x* and *data_y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.losses import numeric_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9741fd",
   "metadata": {},
   "source": [
    "First we need to porperly configured the loss function in such a way that only can receives the *weights*, *data_x* and *data_y*. To do that we can use the **lambda** Python functionality as shown in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65285303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going  to set all the arguments except the *weights*, *data_x* and *data_y*\n",
    "loss_function_ = lambda w_, x_, y_ : qdml_loss_workflow(\n",
    "    w_, x_, y_, dask_client=None, **workflow_cfg)\n",
    "\n",
    "loss_function_mse = lambda w_, x_, y_ : mse_workflow(\n",
    "    w_, x_, y_, dask_client=None, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc97201",
   "metadata": {},
   "source": [
    "Now we can provide the before lambda functions to the *numeric_gradient* for computing the corresponding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gradients_loss = numeric_gradient(weights_, x_train, y_train, loss_function_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb644788",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gradients_mse = numeric_gradient(weights_, x_train, y_train, loss_function_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c26dfe",
   "metadata": {},
   "source": [
    "#### DASK client\n",
    "\n",
    "The computation of gradients involves a lot of executions. In this case, a *DASK* cluster can speed up the computations dramatically. You need to configure properly the loss functions workflows for using a *DASK* cluster to speed up the computations.\n",
    "\n",
    "**BE AWARE**\n",
    "\n",
    "The following cells should be executed only if a *DASK* cluster is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going  to set all the arguments except the *weights*, *data_x* and *data_y*\n",
    "loss_function_dask = lambda w_, x_, y_ : qdml_loss_workflow(\n",
    "    w_, x_, y_, dask_client=client, **workflow_cfg)\n",
    "\n",
    "loss_function_mse_dask = lambda w_, x_, y_ : mse_workflow(\n",
    "    w_, x_, y_, dask_client=client, **workflow_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gradients_loss_dask = numeric_gradient(weights_, x_train, y_train, loss_function_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(gradients_loss_dask, gradients_loss).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gradients_mse_dask = numeric_gradient(weights_, x_train, y_train, loss_function_mse_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(gradients_mse, gradients_mse_dask).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18f2cf",
   "metadata": {},
   "source": [
    "## 8. Evaluation of PQCs with 2 features\n",
    "\n",
    "\n",
    "Mandatory evaluation of PQCs when dataset has more than 1 feature can be very intensive and a Dask Cluster is recommended for such computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563dcd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.qml4var.data_sets import create_random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08cea1",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6163cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data with several features\n",
    "cfg_2d_random = {\n",
    "    \"n_points_train\": 50, \n",
    "    \"n_points_test\" : 100,\n",
    "    \"minval\" : -np.pi,\n",
    "    \"maxval\" : np.pi,\n",
    "    \"features_number\" : 2\n",
    "}\n",
    "x_train_2d, y_train_2d, x_test_2d, y_test_2d = create_random_data(\n",
    "    **cfg_2d_random\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e24813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Only for 2D\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax1.plot3D(x_train_2d[:, 0], x_train_2d[:, 1], y_train_2d[:, 0], 'o')\n",
    "ax1.plot3D(x_test_2d[:, 0], x_test_2d[:, 1], y_test_2d[:, 0], '-', alpha=0.6)\n",
    "ax1.view_init(elev=21, azim=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564e41d",
   "metadata": {},
   "source": [
    "### get QPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3301bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.utils.benchmark_utils import combination_for_list\n",
    "from QQuantLib.qpu.select_qpu import select_qpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_qpu = \"../../benchmark/qml4var/JSONs/qpu_ideal.json\"\n",
    "with open(json_qpu) as json_file:\n",
    "    qpu_dict = json.load(json_file)\n",
    "qpu_list = combination_for_list(qpu_dict)\n",
    "qpu_dict = qpu_list[0]\n",
    "print(qpu_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49344ca5",
   "metadata": {},
   "source": [
    "### get PQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.architectures import hardware_efficient_ansatz, z_observable, normalize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQC building\n",
    "pqc_cfg_2d = {\n",
    "    \"features_number\" : cfg_2d_random[\"features_number\"],\n",
    "    \"n_qubits_by_feature\" : 2,\n",
    "    \"n_layers\": 3    \n",
    "}\n",
    "# Normalization function\n",
    "base_frecuency, shift_feature = normalize_data(\n",
    "    [cfg_2d_random[\"minval\"]] * cfg_2d_random[\"features_number\"],\n",
    "    [cfg_2d_random[\"maxval\"]] * cfg_2d_random[\"features_number\"],\n",
    "    [-0.5*np.pi] * cfg_2d_random[\"features_number\"],\n",
    "    [0.5*np.pi] * cfg_2d_random[\"features_number\"]   \n",
    ")\n",
    "pqc_cfg_2d.update({\n",
    "    \"base_frecuency\" : base_frecuency,\n",
    "    \"shift_feature\" : shift_feature    \n",
    "})   \n",
    "print(pqc_cfg_2d)\n",
    "pqc_2d, weights_names_2d, features_names_2d = hardware_efficient_ansatz(**pqc_cfg_2d)\n",
    "observable_2d = z_observable(**pqc_cfg_2d)\n",
    "# initialize weights\n",
    "weights_2d= [np.random.rand() for w in weights_names_2d]\n",
    "discretization_points = 100\n",
    "nbshots = 0\n",
    "workflow_cfg_2d = {\n",
    "    \"pqc\" : pqc_2d,\n",
    "    \"observable\" : observable_2d,\n",
    "    \"weights_names\" : weights_names_2d,\n",
    "    \"features_names\" : features_names_2d,\n",
    "    \"nbshots\" : nbshots,\n",
    "    \"qpu_info\" : qpu_dict,\n",
    "    \"minval\" : [cfg_2d_random[\"minval\"]] * cfg_2d_random[\"features_number\"],\n",
    "    \"maxval\" : [cfg_2d_random[\"maxval\"]] * cfg_2d_random[\"features_number\"],\n",
    "    \"points\" : discretization_points,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e28a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PQC\n",
    "circuit = pqc_2d.to_circ()\n",
    "circuit.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae127ab",
   "metadata": {},
   "source": [
    "### Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e930de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QQuantLib.qml4var.myqlm_workflows import qdml_loss_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be81bb",
   "metadata": {},
   "source": [
    "### dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975734cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "path_to_schedule_json = \"/home/cesga/gferro/Codigo/qlm_cVar/dask_cluster_ft3/scheduler_info.json\"\n",
    "#path_to_schedule_json = \"/home/cesga/gferro/Codigo/dask_cluster_ft3/scheduler_info.json\"\n",
    "client = Client(\n",
    "    scheduler_file = path_to_schedule_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure loss_function_dask\n",
    "loss_function_dask = lambda w_, x_, y_ : qdml_loss_workflow(\n",
    "    w_, x_, y_, dask_client=client, **workflow_cfg_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7529e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_2d_dask = loss_function_dask(weights_2d, x_train_2d, y_train_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_2d_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_no_dask = lambda w_, x_, y_ : qdml_loss_workflow(\n",
    "    w_, x_, y_, **workflow_cfg_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_2d_no_dask = loss_function_no_dask(weights_2d, x_train_2d, y_train_2d)\n",
    "print(loss_2d_no_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed495ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
